{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 1: Introducción a LangChain\n",
        "\n",
        "**LangChain** es un framework diseñado para crear aplicaciones que aprovechen las capacidades de los modelos de lenguaje grandes (*LLMs*, por sus siglas en inglés).  \n",
        "\n",
        "Su objetivo principal es facilitar la integración de LLMs en flujos de trabajo complejos, brindando componentes y abstracciones que permitan a los desarrolladores construir aplicaciones robustas y escalables. Estas aplicaciones pueden ir desde tareas de generación de texto, respuesta a preguntas y resúmenes, hasta chatbots y sistemas autónomos más avanzados.\n",
        "\n",
        "\n",
        "\n",
        "## Características Principales\n",
        "\n",
        "1. **Componentes Modulares**  \n",
        "   - LangChain se compone de bloques intercambiables y reutilizables, como *cadenas (chains), prompts, herramientas (tools), memorias (memory)* y *agentes (agents)*.  \n",
        "   - Esta modularidad permite construir soluciones específicas o flujos de trabajo complejos, combinando diferentes piezas según tus necesidades.\n",
        "\n",
        "2. **Integración con Fuentes de Datos**  \n",
        "   - Se integra con documentos externos, bases de datos y APIs para enriquecer el contexto de las respuestas generadas por el LLM.  \n",
        "   - Usa *Document Loaders* y *VectorStores* para procesar y buscar información de manera eficiente.\n",
        "\n",
        "3. **Marcos de Agentes**  \n",
        "   - Ofrece herramientas para crear agentes que “razonan” y “actúan” de acuerdo a la retroalimentación o las metas establecidas.  \n",
        "   - Estos agentes pueden encadenar tareas, recurrir a herramientas externas (como buscadores) y mantener el estado de la conversación.\n",
        "\n",
        "4. **Ingeniería de Prompts**  \n",
        "   - Facilita la creación y experimentación de *prompts*, pudiendo usar variables dinámicas y plantillas para personalizar instrucciones.  \n",
        "   - Esencial para guiar al LLM hacia la generación de resultados más pertinentes.\n",
        "\n",
        "## Casos de Uso\n",
        "\n",
        "1. **Chatbots y Agentes Conversacionales**  \n",
        "   - Creación de asistentes virtuales que ofrezcan respuestas coherentes y contextuales a lo largo de múltiples intercambios.\n",
        "\n",
        "2. **Sistemas de Respuesta a Preguntas**  \n",
        "   - Integración de información desde bases de datos o documentos para contestar consultas de manera precisa.\n",
        "\n",
        "3. **Herramientas de Resumen**  \n",
        "   - Capacidad de condensar documentos extensos en resúmenes, informes o puntos clave.\n",
        "\n",
        "4. **Clasificación y Extracción de Datos**  \n",
        "   - Uso de LLMs para etiquetar, extraer entidades o categorizar textos en grandes volúmenes de documentos.\n",
        "\n",
        "## Recomendaciones Generales\n",
        "\n",
        "  \n",
        "- **Estructura Modular**  \n",
        "  Divide tu aplicación en componentes (cadenas, herramientas, memorias) que sean fáciles de probar y mantener.\n",
        "  \n",
        "- **Experimentación Iterativa**  \n",
        "  Prueba diferentes LLMs para encontrar la configuración que mejor funcione. El siguiente link contiene los [modelos de OpenAI](https://platform.openai.com/docs/models) disponibles.\n",
        "  \n",
        "- **Documentación Oficial**  \n",
        "  Explora la [Documentación de LangChain](https://python.langchain.com/) para conocer las funcionalidades más recientes, ejemplos prácticos y mejores prácticas.\n",
        "\n"
      ],
      "metadata": {
        "id": "Avhgr--Wx_p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo Práctico: Respondedor de Preguntas con LangChain\n",
        "\n",
        "En este ejemplo, veremos cómo utilizar **LangChain** para responder preguntas de forma sencilla:\n",
        "\n",
        "1. Cargar `API-KEY`.\n",
        "2. Definir un *prompt template* para estructurar la pregunta.\n",
        "3. Configurar el modelo de lenguaje (*ChatOpenAI*).\n",
        "4. Crear una cadena con el *prompt* y el modelo.\n",
        "5. Invocar la cadena para obtener una respuesta y mostrarla en pantalla.\n"
      ],
      "metadata": {
        "id": "nS77k435nhyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. instalar paquetes necesarios\n"
      ],
      "metadata": {
        "id": "3TllXuZa14Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv langchain_openai\n",
        "\n",
        "# Nota: Si usas \"langchain\" oficial, reemplaza \"langchain_openai\" por \"langchain.chat_models\" u otro import apropiado."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzvgokcAmuih",
        "outputId": "8ca0a738-d7e5-4a2f-ef71-96be486230c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.35)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.3.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.6 python-dotenv-1.0.1 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importar las bibliotecas necesarias\n"
      ],
      "metadata": {
        "id": "7DSBrUXJ17l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "TYIunmOFnxpf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1. Proporcionar directamente las claves de API\n"
      ],
      "metadata": {
        "id": "K5oxrurU1-BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "print(\"Claves de API configuradas correctamente.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTAhyB0aoIt8",
        "outputId": "4b47d9ae-3b0a-4697-8b0c-1f39fc78b661"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claves de API configuradas correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Crear una template de prompt\n"
      ],
      "metadata": {
        "id": "qkMqIzkN2Ai2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Eres un asistente de IA. Responde a la siguiente pregunta de manera concisa y precisa:\n",
        "Pregunta: {pregunta}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"pregunta\"])"
      ],
      "metadata": {
        "id": "zjSywBPYnsKN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Inicializar un modelo de lenguaje (LLM)\n",
        "\n",
        "Prueba diferentes LLMs para encontrar la configuración que mejor funcione. El siguiente link contiene los [modelos de OpenAI](https://platform.openai.com/docs/models) disponibles."
      ],
      "metadata": {
        "id": "OIvWFteO2CT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajusta los parámetros según tus necesidades.\n",
        "llm = ChatOpenAI(model='gpt-4o-2024-11-20', temperature=0.0)"
      ],
      "metadata": {
        "id": "X7dFTdGUoUj6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Crear la cadena (LLMChain)\n",
        "\n",
        "La siguiente instrucción crea una cadena en `LangChain` que toma un `prompt`y lo conecta a un modelo de lenguaje (`llm`), permitiendo generar respuestas automatizadas."
      ],
      "metadata": {
        "id": "4q5-IdIB2ER3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erm2j6-5oW6t",
        "outputId": "cac157be-81cc-4dd2-a1a3-7df41dd1b6da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-726db833e4be>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Función de ejemplo para invocar la cadena\n"
      ],
      "metadata": {
        "id": "SFNxLKrg2GQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_chain():\n",
        "    pregunta = \"¿Qué es LangChain y para qué se usa?\"\n",
        "    output = chain.invoke({\"pregunta\": pregunta})\n",
        "    respuesta = output[\"text\"]\n",
        "\n",
        "    print(\"Pregunta: \", pregunta)\n",
        "    print(\"Respuesta:\", respuesta)\n",
        "\n",
        "# Llamamos a la función para ver el resultado\n",
        "if __name__ == \"__main__\":\n",
        "    ejecutar_chain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwr6cbCXoZEM",
        "outputId": "e76a9c3e-da84-4c07-e594-1645fbd397ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta:  ¿Qué es LangChain y para qué se usa?\n",
            "Respuesta: LangChain es un marco de desarrollo diseñado para crear aplicaciones impulsadas por modelos de lenguaje, como chatbots, asistentes virtuales o sistemas de generación de texto. Permite integrar modelos de lenguaje con fuentes de datos externas (APIs, bases de datos, etc.) y gestionar flujos de trabajo complejos mediante cadenas de procesamiento, facilitando la construcción de aplicaciones más dinámicas e interactivas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "¡Con esto concluye la introducción a **LangChain** y un ejemplo básico de implementación! Te animo a experimentar y explorar las múltiples opciones que ofrece este framework para aprovechar todo el potencial de los modelos de lenguaje grandes.\n"
      ],
      "metadata": {
        "id": "-hrRqEcQot4E"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}