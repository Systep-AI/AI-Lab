{"cells":[{"cell_type":"markdown","metadata":{"id":"Avhgr--Wx_p5"},"source":["# **Resumidor**\n","\n","Este notebook muestra cómo generar un una minuta resumen de tres cartas relacionadas con el sistema eléctrico chileno, utilizando **LangChain** y un modelo de lenguaje.  \n","\n","El resumen se basa en un *prompt* detallado que exige una síntesis clara, concisa e imparcial, siguiendo las reglas específicas descritas en `prompt_summarizer`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sEYzR9stI8xd"},"source":["# 1. Instalación de dependencias y librerías\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bi5Q5tusyB4O","outputId":"2fbbeec4-8fa8-4def-d5b3-06cd26a637ff","executionInfo":{"status":"ok","timestamp":1740060953393,"user_tz":180,"elapsed":18548,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_openai\n","  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.35)\n","Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.61.1)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.3.8)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (4.12.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.10.6)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n","Downloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n","Successfully installed langchain_openai-0.3.6 tiktoken-0.9.0\n","Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["!pip install langchain_openai\n","!pip install --upgrade gdown"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JU-UKUoQIbT1","executionInfo":{"status":"ok","timestamp":1740060965769,"user_tz":180,"elapsed":9717,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"outputs":[],"source":["import os\n","\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI"]},{"cell_type":"code","source":["def read_txt(path):\n","    with open(path, 'r', encoding='utf-8') as file:\n","        return file.read()"],"metadata":{"id":"sXap_paebmGp","executionInfo":{"status":"ok","timestamp":1740060966658,"user_tz":180,"elapsed":13,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YDPk3vqWJCJa"},"source":["# 2. Configuración de variables de entorno\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Cy-_1129HlaL","executionInfo":{"status":"ok","timestamp":1740060973495,"user_tz":180,"elapsed":8,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"outputs":[],"source":["os.environ[\"OPENAI_API_KEY\"] = \"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\""]},{"cell_type":"markdown","metadata":{"id":"80ffSgpzJSPn"},"source":["# 3. Definición del prompt de resumen\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QbxTHqxHobk","outputId":"56379716-0810-4daa-aaf1-734e48730cdf","executionInfo":{"status":"ok","timestamp":1740060974650,"user_tz":180,"elapsed":24,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt cargado correctamente.\n"]}],"source":["prompt_summarizer = \"\"\"\n","# Función\n","Resume el contenido de los siguientes documentos en una minuta. El texto que elabores debe tener en cuenta los siguientes puntos:\n","1. La información proviene de un OCR  y podría o no estar en formato markdown.\n","2. Elabora una minuta lo más extensa posible, maximizando la cantidad de detalles.\n","3. Tus analisis deben ser neutrales y objetivos.\n","4. Usa markdown para estructurar tu respuesta.\n","\n","# Documento 1\n","{doc_1}\n","\n","# Documento 2\n","{doc_2}\n","\n","# Documento 3\n","{doc_3}\n","\n","# Respuesta\n","\"\"\"\n","\n","print(\"Prompt cargado correctamente.\")\n"]},{"cell_type":"markdown","metadata":{"id":"cWYnOXlHJZK9"},"source":["# 4. Clase Summarizer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"HJUS5rpyHs2p","executionInfo":{"status":"ok","timestamp":1740060979020,"user_tz":180,"elapsed":8,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}}},"outputs":[],"source":["class Summarizer:\n","    \"\"\"\n","    Clase que combina un prompt predefinido (prompt_summarizer)\n","    con un modelo de lenguaje para generar un resumen especializado.\n","    \"\"\"\n","    def __init__(self, model=\"gpt-4o-2024-11-20\", temperature=0):\n","        \"\"\"\n","        model: str con el nombre del modelo OpenAI (ej. gpt-4o-2024-11-20).\n","        temperature: Control de 'creatividad' (0 -> más determinista).\n","        \"\"\"\n","        # Definimos el LLM de OpenAI\n","        llm = ChatOpenAI(model=model, temperature=temperature)\n","\n","        # Creamos una plantilla (PromptTemplate) que usa 'prompt_summarizer'\n","        # e introduce las variables \"doc_1\", \"doc_2\", \"doc_3\" donde irán los textos a resumir\n","        prompt = PromptTemplate(\n","            template=prompt_summarizer,\n","            input_variables=[\"doc_1\", \"doc_2\", \"doc_3\"] # input_variables\n","        )\n","\n","        # Encadenamos el prompt y el modelo en un solo paso\n","        self.chain = prompt | llm\n","\n","    def __call__(self, input_texts):\n","        \"\"\"\n","        Llama a la cadena (prompt + modelo) con uno o varios textos a resumir.\n","        Retorna el resumen generado.\n","        \"\"\"\n","\n","        # Ejecutamos la cadena con los textos correspondientes a doc_1, doc_2 y doc_3\n","        output = self.chain.invoke({\"doc_1\": input_texts[0], \"doc_2\": input_texts[1], \"doc_3\": input_texts[2]})\n","\n","        return output.content"]},{"cell_type":"markdown","metadata":{"id":"A1kLae06JfNn"},"source":["# 5. Ejemplo de uso\n","\n","Prueba diferentes LLMs para encontrar la configuración que mejor funcione. El siguiente link contiene los [modelos de OpenAI](https://platform.openai.com/docs/models) disponibles.\n"]},{"cell_type":"code","source":["folder_id = \"1lcNwzeXpfCMMcwpA2ojsdCLCkJF9O_8d\"\n","!gdown --folder https://drive.google.com/drive/folders/{folder_id}"],"metadata":{"id":"bLFEVjI8lGyq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740060994273,"user_tz":180,"elapsed":11703,"user":{"displayName":"Camilo Gutierrez","userId":"18212724788807677958"}},"outputId":"e55020b2-b5ea-4ef8-f6d9-c8e36d4b17dd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieving folder contents\n","Processing file 1MTkrbDj2D_fAyduOlH7pLX_ylg8jwDpU doc_1.md\n","Processing file 16fCfxK3fvvGvvJApF42ZQqJzOjv5k-eb doc_2.txt\n","Processing file 1CEdk4t4GUDvOkci9W_mf2N06CL-tygCy doc_3.txt\n","Retrieving folder contents completed\n","Building directory structure\n","Building directory structure completed\n","Downloading...\n","From: https://drive.google.com/uc?id=1MTkrbDj2D_fAyduOlH7pLX_ylg8jwDpU\n","To: /content/documentos_prueba/doc_1.md\n","100% 3.87k/3.87k [00:00<00:00, 16.6MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=16fCfxK3fvvGvvJApF42ZQqJzOjv5k-eb\n","To: /content/documentos_prueba/doc_2.txt\n","100% 1.63k/1.63k [00:00<00:00, 8.02MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1CEdk4t4GUDvOkci9W_mf2N06CL-tygCy\n","To: /content/documentos_prueba/doc_3.txt\n","100% 1.31k/1.31k [00:00<00:00, 5.82MB/s]\n","Download completed\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEotgSk0HxDp"},"outputs":[],"source":["# Creamos una instancia del resumidor con GPT-4o y temperatura=0 (más determinista)\n","summarizer = Summarizer(model=\"gpt-4o-2024-11-20\", temperature=0)\n","\n","# Debes importar los documentos a tu carpeta principal en colab\n","doc_1 = read_txt(\"/content/documentos_prueba/doc_1.md\")\n","doc_2 = read_txt(\"/content/documentos_prueba/doc_2.txt\")\n","doc_3 = read_txt(\"/content/documentos_prueba/doc_3.txt\")\n","\n","input_docs = [doc_1, doc_2, doc_3]\n","\n","# Llamamos a la clase con el texto a resumir\n","resumen_generado = summarizer(input_docs)\n","\n","print(\"=== RESUMEN GENERADO === \\n\")\n","print(resumen_generado)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/lancedb/vectordb-recipes/blob/main/tutorials/Advace_RAG_LlamaParser/main.ipynb","timestamp":1726083614122}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}